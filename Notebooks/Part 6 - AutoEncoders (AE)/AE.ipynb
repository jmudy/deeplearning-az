{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"3YsQ0VYOtoCt"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hd7dRHyGk59L"},"source":["# Importar las librerías"]},{"cell_type":"code","metadata":{"id":"a-h2dwXIkt5w"},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","from torch.autograd import Variable"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DI6WdEzbk-Za"},"source":["# Importar el dataset\n"]},{"cell_type":"code","metadata":{"id":"Xoeia18zk9jr"},"source":["movies = pd.read_csv(\"/content/deeplearning-az/datasets/Part 6 - AutoEncoders (AE)/ml-1m/movies.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n","users  = pd.read_csv(\"/content/deeplearning-az/datasets/Part 6 - AutoEncoders (AE)/ml-1m/users.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n","ratings  = pd.read_csv(\"/content/deeplearning-az/datasets/Part 6 - AutoEncoders (AE)/ml-1m/ratings.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KSbdp-wUlFIX"},"source":["# Preparar el conjunto de entrenamiento y elconjunto de testing"]},{"cell_type":"code","metadata":{"id":"70yEO3RWlBg2"},"source":["training_set = pd.read_csv(\"/content/deeplearning-az/datasets/Part 6 - AutoEncoders (AE)/ml-100k/u1.base\", sep = \"\\t\", header = None)\n","training_set = np.array(training_set, dtype = \"int\")\n","test_set = pd.read_csv(\"/content/deeplearning-az/datasets/Part 6 - AutoEncoders (AE)/ml-100k/u1.test\", sep = \"\\t\", header = None)\n","test_set = np.array(test_set, dtype = \"int\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twATLFWTlMKM"},"source":["# Obtener el número de usuarios y de películas"]},{"cell_type":"code","metadata":{"id":"3CGpdosSlHXu"},"source":["nb_users = int(max(max(training_set[:, 0]), max(test_set[:,0])))\n","nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tSC4LbuvlT_I"},"source":["# Convertir los datos en un array X[u,i] con usuarios u en fila y películas i en columna\n"]},{"cell_type":"code","metadata":{"id":"VmJQ-0fDlJZ4"},"source":["def convert(data):\n","    new_data = []\n","    for id_user in range(1, nb_users+1):\n","        id_movies = data[:, 1][data[:, 0] == id_user]\n","        id_ratings = data[:, 2][data[:, 0] == id_user]\n","        ratings = np.zeros(nb_movies)\n","        ratings[id_movies-1] = id_ratings\n","        new_data.append(list(ratings))\n","    return new_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jno3ahx9lXB3"},"source":["training_set = convert(training_set)\n","test_set = convert(test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q7n1NZqylbCO"},"source":["# Convertir los datos a tensores de Torch"]},{"cell_type":"code","metadata":{"id":"qTRjUdQLlYdP"},"source":["training_set = torch.FloatTensor(training_set)\n","test_set = torch.FloatTensor(test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0JAFQYBflfqd"},"source":["# Crear la arquitectura de la Red Neuronal"]},{"cell_type":"code","metadata":{"id":"YbDUudP_ldDe"},"source":["class SAE(nn.Module):\n","    def __init__(self, ):\n","        super(SAE, self).__init__()\n","        self.fc1 = nn.Linear(nb_movies, 20)\n","        self.fc2 = nn.Linear(20, 10)\n","        self.fc3 = nn.Linear(10, 20)\n","        self.fc4 = nn.Linear(20, nb_movies)\n","        self.activation = nn.Sigmoid()\n","    def forward(self, x):\n","        x = self.activation(self.fc1(x))\n","        x = self.activation(self.fc2(x))\n","        x = self.activation(self.fc3(x))\n","        x = self.fc4(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TC6XXcLklhrU"},"source":["sae = SAE()\n","criterion = nn.MSELoss()\n","optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EqRbcjMIlo1P"},"source":["# Entrenar el SAE"]},{"cell_type":"code","metadata":{"id":"gy_6yKN7lkAd","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"921a8746-6d44-4745-99fc-bd140fcd2510"},"source":["nb_epoch = 200\n","for epoch in range(1, nb_epoch+1):\n","    train_loss = 0\n","    s = 0.\n","    for id_user in range(nb_users):\n","        input = Variable(training_set[id_user]).unsqueeze(0)\n","        target = input.clone()\n","        if torch.sum(target.data > 0) > 0:\n","            output = sae.forward(input)\n","            target.require_grad = False\n","            output[target == 0] = 0\n","            loss = criterion(output, target)\n","            # la media no es sobre todas las películas, sino sobre las que realmente ha valorado\n","            mean_corrector = nb_movies/float(torch.sum(target.data > 0)+1e-10)\n","            loss.backward()\n","            train_loss += np.sqrt(loss.data*mean_corrector) ## sum(errors) / n_pelis_valoradas\n","            s += 1.\n","            optimizer.step()\n","    print(\"Epoch: \"+str(epoch)+\", Loss: \"+str(train_loss/s))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 1, Loss: tensor(1.7711)\n","Epoch: 2, Loss: tensor(1.0967)\n","Epoch: 3, Loss: tensor(1.0534)\n","Epoch: 4, Loss: tensor(1.0385)\n","Epoch: 5, Loss: tensor(1.0307)\n","Epoch: 6, Loss: tensor(1.0267)\n","Epoch: 7, Loss: tensor(1.0237)\n","Epoch: 8, Loss: tensor(1.0218)\n","Epoch: 9, Loss: tensor(1.0209)\n","Epoch: 10, Loss: tensor(1.0196)\n","Epoch: 11, Loss: tensor(1.0189)\n","Epoch: 12, Loss: tensor(1.0183)\n","Epoch: 13, Loss: tensor(1.0179)\n","Epoch: 14, Loss: tensor(1.0172)\n","Epoch: 15, Loss: tensor(1.0174)\n","Epoch: 16, Loss: tensor(1.0169)\n","Epoch: 17, Loss: tensor(1.0168)\n","Epoch: 18, Loss: tensor(1.0164)\n","Epoch: 19, Loss: tensor(1.0165)\n","Epoch: 20, Loss: tensor(1.0162)\n","Epoch: 21, Loss: tensor(1.0162)\n","Epoch: 22, Loss: tensor(1.0159)\n","Epoch: 23, Loss: tensor(1.0160)\n","Epoch: 24, Loss: tensor(1.0160)\n","Epoch: 25, Loss: tensor(1.0159)\n","Epoch: 26, Loss: tensor(1.0156)\n","Epoch: 27, Loss: tensor(1.0154)\n","Epoch: 28, Loss: tensor(1.0150)\n","Epoch: 29, Loss: tensor(1.0126)\n","Epoch: 30, Loss: tensor(1.0110)\n","Epoch: 31, Loss: tensor(1.0095)\n","Epoch: 32, Loss: tensor(1.0086)\n","Epoch: 33, Loss: tensor(1.0052)\n","Epoch: 34, Loss: tensor(1.0052)\n","Epoch: 35, Loss: tensor(1.0013)\n","Epoch: 36, Loss: tensor(0.9998)\n","Epoch: 37, Loss: tensor(0.9959)\n","Epoch: 38, Loss: tensor(0.9956)\n","Epoch: 39, Loss: tensor(0.9939)\n","Epoch: 40, Loss: tensor(0.9936)\n","Epoch: 41, Loss: tensor(0.9902)\n","Epoch: 42, Loss: tensor(0.9897)\n","Epoch: 43, Loss: tensor(0.9847)\n","Epoch: 44, Loss: tensor(0.9847)\n","Epoch: 45, Loss: tensor(0.9861)\n","Epoch: 46, Loss: tensor(0.9861)\n","Epoch: 47, Loss: tensor(0.9834)\n","Epoch: 48, Loss: tensor(0.9864)\n","Epoch: 49, Loss: tensor(0.9829)\n","Epoch: 50, Loss: tensor(0.9835)\n","Epoch: 51, Loss: tensor(0.9800)\n","Epoch: 52, Loss: tensor(0.9848)\n","Epoch: 53, Loss: tensor(0.9829)\n","Epoch: 54, Loss: tensor(0.9824)\n","Epoch: 55, Loss: tensor(0.9747)\n","Epoch: 56, Loss: tensor(0.9707)\n","Epoch: 57, Loss: tensor(0.9680)\n","Epoch: 58, Loss: tensor(0.9715)\n","Epoch: 59, Loss: tensor(0.9688)\n","Epoch: 60, Loss: tensor(0.9671)\n","Epoch: 61, Loss: tensor(0.9624)\n","Epoch: 62, Loss: tensor(0.9646)\n","Epoch: 63, Loss: tensor(0.9666)\n","Epoch: 64, Loss: tensor(0.9665)\n","Epoch: 65, Loss: tensor(0.9606)\n","Epoch: 66, Loss: tensor(0.9623)\n","Epoch: 67, Loss: tensor(0.9579)\n","Epoch: 68, Loss: tensor(0.9579)\n","Epoch: 69, Loss: tensor(0.9536)\n","Epoch: 70, Loss: tensor(0.9554)\n","Epoch: 71, Loss: tensor(0.9566)\n","Epoch: 72, Loss: tensor(0.9552)\n","Epoch: 73, Loss: tensor(0.9504)\n","Epoch: 74, Loss: tensor(0.9541)\n","Epoch: 75, Loss: tensor(0.9506)\n","Epoch: 76, Loss: tensor(0.9505)\n","Epoch: 77, Loss: tensor(0.9481)\n","Epoch: 78, Loss: tensor(0.9497)\n","Epoch: 79, Loss: tensor(0.9461)\n","Epoch: 80, Loss: tensor(0.9466)\n","Epoch: 81, Loss: tensor(0.9443)\n","Epoch: 82, Loss: tensor(0.9464)\n","Epoch: 83, Loss: tensor(0.9437)\n","Epoch: 84, Loss: tensor(0.9446)\n","Epoch: 85, Loss: tensor(0.9418)\n","Epoch: 86, Loss: tensor(0.9433)\n","Epoch: 87, Loss: tensor(0.9412)\n","Epoch: 88, Loss: tensor(0.9417)\n","Epoch: 89, Loss: tensor(0.9411)\n","Epoch: 90, Loss: tensor(0.9419)\n","Epoch: 91, Loss: tensor(0.9389)\n","Epoch: 92, Loss: tensor(0.9392)\n","Epoch: 93, Loss: tensor(0.9373)\n","Epoch: 94, Loss: tensor(0.9386)\n","Epoch: 95, Loss: tensor(0.9363)\n","Epoch: 96, Loss: tensor(0.9380)\n","Epoch: 97, Loss: tensor(0.9351)\n","Epoch: 98, Loss: tensor(0.9368)\n","Epoch: 99, Loss: tensor(0.9348)\n","Epoch: 100, Loss: tensor(0.9363)\n","Epoch: 101, Loss: tensor(0.9347)\n","Epoch: 102, Loss: tensor(0.9356)\n","Epoch: 103, Loss: tensor(0.9339)\n","Epoch: 104, Loss: tensor(0.9352)\n","Epoch: 105, Loss: tensor(0.9325)\n","Epoch: 106, Loss: tensor(0.9340)\n","Epoch: 107, Loss: tensor(0.9319)\n","Epoch: 108, Loss: tensor(0.9332)\n","Epoch: 109, Loss: tensor(0.9312)\n","Epoch: 110, Loss: tensor(0.9324)\n","Epoch: 111, Loss: tensor(0.9309)\n","Epoch: 112, Loss: tensor(0.9320)\n","Epoch: 113, Loss: tensor(0.9303)\n","Epoch: 114, Loss: tensor(0.9314)\n","Epoch: 115, Loss: tensor(0.9296)\n","Epoch: 116, Loss: tensor(0.9300)\n","Epoch: 117, Loss: tensor(0.9288)\n","Epoch: 118, Loss: tensor(0.9297)\n","Epoch: 119, Loss: tensor(0.9283)\n","Epoch: 120, Loss: tensor(0.9293)\n","Epoch: 121, Loss: tensor(0.9278)\n","Epoch: 122, Loss: tensor(0.9286)\n","Epoch: 123, Loss: tensor(0.9272)\n","Epoch: 124, Loss: tensor(0.9286)\n","Epoch: 125, Loss: tensor(0.9265)\n","Epoch: 126, Loss: tensor(0.9273)\n","Epoch: 127, Loss: tensor(0.9264)\n","Epoch: 128, Loss: tensor(0.9271)\n","Epoch: 129, Loss: tensor(0.9258)\n","Epoch: 130, Loss: tensor(0.9264)\n","Epoch: 131, Loss: tensor(0.9254)\n","Epoch: 132, Loss: tensor(0.9258)\n","Epoch: 133, Loss: tensor(0.9244)\n","Epoch: 134, Loss: tensor(0.9252)\n","Epoch: 135, Loss: tensor(0.9247)\n","Epoch: 136, Loss: tensor(0.9253)\n","Epoch: 137, Loss: tensor(0.9236)\n","Epoch: 138, Loss: tensor(0.9244)\n","Epoch: 139, Loss: tensor(0.9230)\n","Epoch: 140, Loss: tensor(0.9227)\n","Epoch: 141, Loss: tensor(0.9225)\n","Epoch: 142, Loss: tensor(0.9225)\n","Epoch: 143, Loss: tensor(0.9217)\n","Epoch: 144, Loss: tensor(0.9217)\n","Epoch: 145, Loss: tensor(0.9214)\n","Epoch: 146, Loss: tensor(0.9222)\n","Epoch: 147, Loss: tensor(0.9212)\n","Epoch: 148, Loss: tensor(0.9206)\n","Epoch: 149, Loss: tensor(0.9206)\n","Epoch: 150, Loss: tensor(0.9212)\n","Epoch: 151, Loss: tensor(0.9208)\n","Epoch: 152, Loss: tensor(0.9211)\n","Epoch: 153, Loss: tensor(0.9202)\n","Epoch: 154, Loss: tensor(0.9202)\n","Epoch: 155, Loss: tensor(0.9195)\n","Epoch: 156, Loss: tensor(0.9195)\n","Epoch: 157, Loss: tensor(0.9192)\n","Epoch: 158, Loss: tensor(0.9192)\n","Epoch: 159, Loss: tensor(0.9184)\n","Epoch: 160, Loss: tensor(0.9190)\n","Epoch: 161, Loss: tensor(0.9187)\n","Epoch: 162, Loss: tensor(0.9182)\n","Epoch: 163, Loss: tensor(0.9178)\n","Epoch: 164, Loss: tensor(0.9180)\n","Epoch: 165, Loss: tensor(0.9178)\n","Epoch: 166, Loss: tensor(0.9177)\n","Epoch: 167, Loss: tensor(0.9175)\n","Epoch: 168, Loss: tensor(0.9174)\n","Epoch: 169, Loss: tensor(0.9169)\n","Epoch: 170, Loss: tensor(0.9174)\n","Epoch: 171, Loss: tensor(0.9165)\n","Epoch: 172, Loss: tensor(0.9167)\n","Epoch: 173, Loss: tensor(0.9161)\n","Epoch: 174, Loss: tensor(0.9168)\n","Epoch: 175, Loss: tensor(0.9160)\n","Epoch: 176, Loss: tensor(0.9164)\n","Epoch: 177, Loss: tensor(0.9159)\n","Epoch: 178, Loss: tensor(0.9160)\n","Epoch: 179, Loss: tensor(0.9158)\n","Epoch: 180, Loss: tensor(0.9158)\n","Epoch: 181, Loss: tensor(0.9152)\n","Epoch: 182, Loss: tensor(0.9155)\n","Epoch: 183, Loss: tensor(0.9150)\n","Epoch: 184, Loss: tensor(0.9152)\n","Epoch: 185, Loss: tensor(0.9148)\n","Epoch: 186, Loss: tensor(0.9150)\n","Epoch: 187, Loss: tensor(0.9147)\n","Epoch: 188, Loss: tensor(0.9142)\n","Epoch: 189, Loss: tensor(0.9148)\n","Epoch: 190, Loss: tensor(0.9150)\n","Epoch: 191, Loss: tensor(0.9143)\n","Epoch: 192, Loss: tensor(0.9143)\n","Epoch: 193, Loss: tensor(0.9138)\n","Epoch: 194, Loss: tensor(0.9143)\n","Epoch: 195, Loss: tensor(0.9135)\n","Epoch: 196, Loss: tensor(0.9138)\n","Epoch: 197, Loss: tensor(0.9132)\n","Epoch: 198, Loss: tensor(0.9136)\n","Epoch: 199, Loss: tensor(0.9128)\n","Epoch: 200, Loss: tensor(0.9132)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ENHGLCyiluJn"},"source":["# Evaluar el conjunto de test en nuestro SAE"]},{"cell_type":"code","metadata":{"id":"UAL0ZZOhlruw"},"source":["test_loss = 0\n","s = 0.\n","for id_user in range(nb_users):\n","    input = Variable(training_set[id_user]).unsqueeze(0)\n","    target = Variable(test_set[id_user]).unsqueeze(0)\n","    if torch.sum(target.data > 0) > 0:\n","        output = sae.forward(input)\n","        target.require_grad = False\n","        output[target == 0] = 0\n","        loss = criterion(output, target)\n","        # la media no es sobre todas las películas, sino sobre las que realmente ha valorado\n","        mean_corrector = nb_movies/float(torch.sum(target.data > 0)+1e-10)\n","        test_loss += np.sqrt(loss.data*mean_corrector) ## sum(errors) / n_pelis_valoradas\n","        s += 1."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Quy4q1UFlwfF","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"dbece7af-feb4-4943-aaf0-d4b9f8fc6164"},"source":["print(\"Test Loss: \"+str(test_loss/s))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss: tensor(0.9549)\n"],"name":"stdout"}]}]}